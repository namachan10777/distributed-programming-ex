@require: coins/report
@require: enumitem/enumitem

open EnumitemAlias in
document(|
  author = {Masaki Nakano};
  department = {情報科学};
  en = false;
  id = 202013553;
  faculty = {情報};
  title = {Raft};
|) '<
  +chapter?:(`raft`) ?:(`raft`) ?:(3) {Raft}<
    +section{概要}<
      +p{
        Raftは分散合意プロトコルであり、理解しやすく実用的で複数ノードでステートマシンへの
        コマンド列の同期を取ることを目的としている。
      }
      +p{
        Raft以前にはLamportらにより提案されたPaxosアルゴリズムが広く利用されていたが、
        Paxosは難解であり実装及び理解が難しく、
        基本プロトコルは実用的でないためMulti-Paxosと呼ばれる複数のPaxosインスタンスの組み合わせを用いるが、
        これに広く合意されたアルゴリズムがない事を筆者らは問題であると主張している。
        また、PaxosはP2Pにより合意を取っていくが
        現実的にはリーダを選出した方が単純であり高速に動作すると主張している。
      }
      +p{
        GoogleのChubbyはMulti-Paxosを利用して構築されている。
        Consul、etcdはRaftである。
        分散型でトランザクションをサポートするKVSであるTiKVはRaftを利用している。
      }
      +p{
        ここでは通信経路、メッセージ、ステートマシンの抽象化による利用可能性を捨て、
        なるべく単純なアプローチでログ複製を実装することでプロトコルへの理解を目的として実装する。
      }
    >
    +section{設計}<
      +p{
        基本はRaftの論文と同様の実装とした。
        クラスタのメンバーシップ変更は実装していない。
      }
      +p{
        \code(`AppendEntries`); RPCは原論文ではリーダー就任直後は
        空のものを送るとしており多くの解説でも同様のことが言われているが、
        単純に再選挙を防止するためのものであるので
        空である必要はない。
        Raftの原論文では殆どの場合リーダーとフォロワーのログは一致していることから、
        実際には空の\code(`AppendEntries`);が送られることになるため、
        そのような記述になっていると思われる。
        またハートビートも同様に空である必要はない。
        そのため実装の単純化のために常に\code(`AppendEntries`); RPCはリーダーのログを複製することを
        試みるようにした。
      }
      +p{
        また、原論文ではリーダーから見える全てのフォロワーがログの複製に成功するまで
        \code(`AppendEntries`);を繰り返すとしている。
        しかしフォロワーが長期間故障するような状態では
        すぐに\code(`AppendEntries`);を実施したのではあまり意味のないRPCが大量に
        発行されることになる。
        Raftの\code(`AppendEntries`);では複数のログを送れるのでフォロワーが復帰次第
        僅かなRPCで複製が完了することが期待できる事、
        ハートビートの間隔が短いことからハートビートをリトライに用いることとした。
        前述の通りハートビートは常に最新のログの複製を試みるのでこの方法で実装可能である。
        以上が原論文の実装との差異である。
      }
    >
    +section{実装}<
      +p{
        RPCはgRPCではなく単純なREST APIとした。
        ログはテキストのみとした。
        今回でも同様に\code(`tokio`);ランタイムを用い、
        Webアプリケーションフレームワークの\code(`warp`);とHTTPクライアントの\code(`reqwest`);
        を利用した。
        Raftでは一部の状態は不揮発な部分に永続化することが求められる。
        本来は事前書き込みログ(WAL: Write ahead log)等を利用し不整合を防ぎつつ永続化するべきであるが、
        単純化のためにJSONファイルで実装した。
      }
      +p{
        Raft内部の状態はデッドロックを避けるためジャイアントロックにより管理した。
        例外はリーダーのみの持つ揮発性のデータである\code(`next_index`);と\code(`match_index`);であるが
        これはアクセスする状況が限定されており
        デッドロックを起こさない操作列を実装しやすいため許容した。
        ジャイアントロックであるためIOを跨いでロックを保持すると
        パフォーマンスに悪影響を与えるので、
        IO前後でロックの解除を行う。
        この時にIO前後で変数が変更される可能性があるので条件について検証を行った。
      }
      +subsection{\code(`AppendEntries`);におけるログの変更}<
        +p{
          クライアントからのログ追加要求及び\code(`AppendEntries`);の受信の両方で変更がlogsの変更が行われる可能性がある。
          問題となるのはログの変更に関わる部分である。
          \code(`AppendEntries`);はIOを挟んでロックを取り直す。この後にログを用いて\code(`commit_index`);を操作するため不整合が生じる恐れがある。
          ここでログが変更されるのは二種類のシナリオが考えられる
          \listing{
            * 送信-送信競合
              ** これは送信時に操作の全域にわたって取得するロックを用意して解決する
            * 送信-受信競合
              ** これは発生しないことを示す
              ** 受信は自身がリーダーであるかぎり拒否する。受信によって変更されるのは自身がフォロワーに転落した場合である
              ** 実装上送信のタイムアウトはリーダータイムアウトの${\frac{2}{3}}に設定されており、
                 フォロワーに到達した時点でタイムアウトがリセットされるため、
                 フォロワーがリーダーがダウンしたと推測して再選挙を開始する前に必ず送信がタイムアウトする。
                 よって送信前に自身がリーダーであった場合は受信成功時に自身がフォロワーになることはありえない。
              ** そのため受信によりログを変更されることはない
          }
        }
      >
      +subsection{\code(`RequestVote`);における誤った当選}<
        +p{
          \code(`RequestVote`);も途中でロックを手放す。
          この時にログが変更される可能性があるが、
          ログの変更により勝利可能性と実際の勝敗が変わらなかった場合は関係がない。
          食い違った場合もRaftはログの状態による自己判断ではなく
          選挙によってリーダーを選出するので影響を与えない。
        }
        +p{
          途中でCandidateでなくなる場合についてはIO後にチェックを行い
          転落時はLeaderへの就任をやめることで解決する。
        }
        +p{
          途中でtermが増加する場合が考えられる。これは選挙にもちいた
          termと選挙後のtermが食い違うということである。
          termが増加する場合はFollowerに転落するので前述のCandidateかのチェックで
          失敗しLeaderへ就任しないので問題ない。
        }
      >
    >
    +section{動作確認}<
      +p{
        3ノード、5ノードでリーダの就任とログの複製について確認した。
      }
    >
    +section{感想}<
      +p{
        Raft単体では複数マシン間で強い整合性をもったコマンド列を適用することしか出来ず、
        その性能はリーダーのWriteを経由するため制約される。
        またノードが増えすぎると選挙タイムアウトを伸ばさなければ分割投票が発生しやすくなるためスケーラビリティはあまり高くない。
        ただ複数ノード間で強い整合性を出せるのは極めて便利であるし、
        レイテンシを許容すれば階層化でスケールさせることは可能である。
        この結果と汎用性を比較的単純なアルゴリズムで実現したことに驚きがあった。
      }
      +p{
        ただ、原論文の通りに実装するだけとは言っても実際のRaftノードは並列にリクエストを捌けなければ他の部分でのネットワーク遅延が全体に伝播してしまう。
        それを防ぐためには適宜ロックを取り不正な状態にならないようにしつつ
        並列性を確保しなくてはならないが、変更されても問題のない条件と変更された場合問題のある条件を検討するのは厄介だった。
      }
      +p{
        今回は単純な実装であるしテストも簡易的なものしか実施していないが、
        実装自体は抽象化しやすそうであるので抽象化した上でファジングテストを実施しライブラリとして利用出来るようにしたい。
      }
    >
  >
>