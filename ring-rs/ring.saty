@require: coins/report
@require: enumitem/enumitem

open EnumitemAlias in
document(|
  author = {Masaki Nakano};
  department = {情報科学};
  en = false;
  id = 202013553;
  faculty = {情報};
  title = {Ringアルゴリズム};
|) '<
  +chapter ?:(`ring`) ?:(`ring`) ?:(1) {Ringアルゴリズム}<
    +section{概要}<
      +p{
        Ringアルゴリズムは環状にノード同士を接続し、
        ノード管理を行う分散アルゴリズムである。
        各ノードは環状の次のノードのアドレスを保持する変数(以下\code(`next`);)
        と、前のノードのアドレスを保持する変数（以下\code(`prev`);)を持ち、
        ノード同士でこの変数を操作しあうことで接続を保持し管理する。
      }
      +p{
        基本的には各ノードは対等な関係として実装可能だが、
        実際の分散システムではRingでただ接続するのではなく
        他の分散処理を行う際にメンバーシップを管理するために導入される場合がある。
        そのような場合はLeader-Follower replicationのように
        マスターノードを選出する場合がある。
        また、接続喪失時の回復などマスターノードが存在した方が実装しやすい
        追加機能もある事から、リーダー選出も求められる機能である。
      }
    >
    +section{設計}<
      +p{
        ノードの新規参加、退出、リーダーの選出とリーダーの死活監視、接続喪失時の回復処理を実装した。
        簡単のため退出専用の機能は実装せず、接続喪失を検出すると実施される接続回復処理によって実現する。
        実装したRPCは以下の通りである。
      }
      +listing {
        * \code(`set_next`);
        * \code(`set_prev`);
        * \code(`join`);
        * \code(`list`);
        * \code(`election`);
        * \code(`coordinate`);
        * \code(`connection_announce`);
      }
      +p{
        通常の新規参加は実験資料のとおり、\code(`join`); RPCを発行し\code(`next`);としたい
        ノードに自信を\code(`prev`);として認識させる。
        この時の\code(`next.prev`);を\code(`join`); の返却値として受け取り
        \code(`next.prev`);に\code(`set_next`); RPCを発行することで
        リングの接続状況を更新する。
      }
      +p{
        メンバーシップの取得及びリーダー選出はそれぞれ\code(`list`);、\code(`election`); RPCで行う。
        \code(`prev`);方向に非同期的にRPCを発行し、
        受け取ったノードは自身を追記し\code(`prev`);に流す。
        送られてきたリストに自身が入っている場合は一周したと判断し、RPCを停止する。
        今回の実装ではリーダーのみが最初の\code(`list`); RPCを発行する。
      }
      +p{
        リーダの選出はまずメンバーシップを収集するところから開始する。
        リーダーから送られるべき\code(`list`); RPCが一定期間送られていないことを認識したノードは
        \code(`election`); RPCを発行してメンバーシップの収集を開始する。
        一周しメンバーシップが収集できた場合は\code(`coordinate`); RPCを発行し接続されているノードに
        メンバーシップ表を通知する。
        リーダーはメンバーシップ表内で辞書順で最も大きい名前を持つノードが就任する。
      }
      +p{
        接続回復処理はリーダーが生存報告のために定期的に行う\code(`list`); RPCにより
        収集されたメンバーシップを\code(`connection_announce`); RPCとして
        メンバー内のノードに通知し接続のキャッシュを各ノードに保存することで行う。
        接続喪失を確認したノードは速やかにキャッシュされた接続表より順番に\code(`set_next`);または\code(`set_prev`);
        により接続の回復を試行する。
      }
      +p{
        リーダーが複数選出されるケースを軽減するため、
        自分以外が発行した\code(`list`); RPCに自分より大きい名前が入っている場合はリーダーを辞任する。
        \code(`list`); RPCはリーダーしか発行しないため、
        単一のリーダーしか居ない状態ではこれによりリーダーの交代が発生することはない。
      }
    >
    +section{実装}<
      +p{
        当初は実装言語に\code(`C`);、RPCに\code(`mochi-margo`);を用いたが、
        途中で\code(`Rust`);と\code(`gRPC`);による実装に切り替えた。
      }
      +p{
        RPCを並行して処理するためマルチスレッドでの処理を行い、
        自身がリーダーかを保存する変数、キャッシュされた接続表、
        自身の接続状況をそれぞれReader-Writerロックにより保護し競合状態を防いでいる。
      }
      +p{
        リーダーからの生存報告のタイムアウトには一定時間毎に\code(`list`); RPCの報告を受け取らない限り
        自動的にリーダー再選出を開始するスレッドを用意し、それを\code(`list`); RPCを受けた時点で解除することで
        実装した。非同期ランタイムライブラリの\code(`tokio`);には
        複数の非同期タスクのうち最も早く終了したタスクの結果をとる\code(`select!`);マクロがあるため、
        これを用い単純なスリープである\code(`tokio::time::sleep()`);
        とスレッド間通信を行う\code(`tokio::sync::mpsc::Reciever`);
        の終了を比較し、\code(`tokio::time::sleep`);が先に終了した場合はリーダー再選出を行う。
      }
      +p{
        また、競合条件の発生が予期される\code(`join`); RPCは同時に複数の\code(`join`); RPCのハンドリングを行わないよう
        ロックをとった。
      }
    >
    +section{動作確認}<
      +p{
        単一のMacOS上で1,2,3,4,5ノードを立てて実験した。
        単一のノードのみしかダウンしない状況ではリーダーがダウンした場合も
        フォロワーがダウンした場合も正しく回復が行われることを確認した。
        複数のノードが同時にダウンするシナリオでもほとんどの場合正常な回復が
        行われるが、再起動と同時ダウンが頻発するシナリオでは
        経路回復に失敗しリングが分断されるケースがあった。
      }
    >
    +section{議論}<
      +p{
        Ringアルゴリズムではクラスタへの参加をアトミックに処理できない為競合条件
        による不正な状態を防ぐのが難しい。
        今回はコーナーケースの検討と想定されるシナリオについての考察が出来ず、\code(`join`); RPCのハンドリングにロックを取ることで
        軽減することしか出来なかったが、
        検討の余地は大きいように思う。
      }
      +p{
        実験では激しい再起動と同時ダウンの頻発により
        リングの分断が発生したが、実際の運用ではサーバの平均故障頻度に対してRPCの通信頻度は十分に大きいことが
        予期されるため実用上の問題は低いと考えられる。
        一方で実験では時間及び技術力の不足によりネットワーク遅延の挿入には失敗している。
        ダウンするケースと異なりネットワーク遅延のケースでは通信不能と判断されたノードは
        ネットワーク障害が復旧すれば状態を保持したまま通信を試みるため不整合が強く発生することが予期される。
        ネットワーク障害を判断し自死するよう設計すればこれは防げるが、
        障害の種別によっては自死しないものの他のノードからは寸断とみなされるケースが考えられる(不規則なパケットロス、遅延など)。
      }
    >
    +section{感想}<
      +p{
        Rustでの実装によりメッセージのシリアライゼーション、
        メモリ競合の防止はミスなく防げたが、デッドロックが頻発しデバッグと対策に追われた。
        単一ノードの場合と違いノード間の相互のRPCによってデッドロックが発生する為デバッグが困難だった。
      }
      +p{
        また、非同期処理のためにRustの非同期ランタイムである\code(`tokio`);を使用したが、
        \code(`tokio`);はtask-stealingを行うため一つの連続したタスクが同じスレッドで実行されるとは限らない。
        結果スタックトレースでもデバッガでも大量のランタイムコードが走るスレッドプールの中に処理が分散し、
        デバッグが困難となった。
      }
    >
  >
>